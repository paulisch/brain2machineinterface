# brain2machineinterface
The goal of this project is to read live EEG data and detect gestures in the incoming signals. Currently supported gestures are "look left", "look right" and "bite". These gestures trigger a events which are then used for controlling a self-built Lego Mindstorms gripper arm.
